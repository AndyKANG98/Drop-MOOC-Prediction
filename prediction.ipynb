{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data/date.csv loaded! Number of courses:', 39)\n",
      "('data/object.csv loaded! Number of moduels:', 26750)\n",
      "('data/train/log_train.csv loaded! Size of log data:', 4677908)\n",
      "('data/train/enrollment_train.csv loaded! Number of enrollments:', 72395)\n",
      "('data/train/truth_train.csv loaded! Number of labels:', 72395)\n",
      "==========train data loading finished==========\n",
      "()\n",
      "('data/date.csv loaded! Number of courses:', 39)\n",
      "('data/object.csv loaded! Number of moduels:', 26750)\n",
      "('data/test/log_test.csv loaded! Size of log data:', 1548480)\n",
      "('data/test/enrollment_test.csv loaded! Number of enrollments:', 24013)\n",
      "('data/test/truth_test.csv loaded! Number of labels:', 24013)\n",
      "==========test data loading finished==========\n",
      "()\n",
      "Event_count extracted! Number of features: 7; time: 1.349625 seconds\n",
      "Weekly_session_count extracted! Number of features: 6; time: 66.430914 seconds\n",
      "Problem_video_ratio extracted! Number of features: 2; time: 3.251933 seconds\n",
      "==========All features extracted==========\n",
      "('Shape of the features dataframe: ', (72395, 16))\n",
      "Finish training data preprocessing! Time: 71.225868 seconds\n",
      "The shape of X: (72395, 15); shape of y: (72395,)\n",
      "()\n",
      "Event_count extracted! Number of features: 7; time: 0.442832 seconds\n",
      "Weekly_session_count extracted! Number of features: 6; time: 21.209639 seconds\n",
      "Problem_video_ratio extracted! Number of features: 2; time: 1.053086 seconds\n",
      "==========All features extracted==========\n",
      "('Shape of the features dataframe: ', (24013, 16))\n",
      "Finish testing data preprocessing! Time: 22.775250 seconds\n",
      "The shape of X: (24013, 15); shape of y: (24013,)\n",
      "()\n",
      "Classification report for classifier RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=12, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=-1,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.55      0.65      4902\n",
      "           1       0.89      0.96      0.92     19111\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     24013\n",
      "   macro avg       0.83      0.76      0.79     24013\n",
      "weighted avg       0.87      0.88      0.87     24013\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 2718  2184]\n",
      " [  795 18316]]\n",
      "Testing Score:\n",
      "0.8759421979760963\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier LogisticRegression(C=1.5, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=0, solver='warn',\n",
      "          tol=0.001, verbose=0, warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.54      0.64      4902\n",
      "           1       0.89      0.96      0.92     19111\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     24013\n",
      "   macro avg       0.84      0.75      0.78     24013\n",
      "weighted avg       0.87      0.87      0.87     24013\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 2624  2278]\n",
      " [  736 18375]]\n",
      "Testing Score:\n",
      "0.8744846541456711\n",
      "()\n",
      "Classification report for classifier XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=3, learning_rate=0.05, max_delta_step=1,\n",
      "       max_depth=6, min_child_weight=2, missing=None, n_estimators=200,\n",
      "       n_jobs=-1, nthread=None, objective='binary:logistic',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.56      0.65      4902\n",
      "           1       0.90      0.96      0.93     19111\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     24013\n",
      "   macro avg       0.83      0.76      0.79     24013\n",
      "weighted avg       0.87      0.88      0.87     24013\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 2765  2137]\n",
      " [  810 18301]]\n",
      "Testing Score:\n",
      "0.8772748094781994\n",
      "()\n",
      "Classification report for classifier VotingClassifier(estimators=[('rf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=12, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=3,\n",
      "            min_weig...tate=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1))],\n",
      "         flatten_transform=None, n_jobs=None, voting='hard', weights=None):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.56      0.65      4902\n",
      "           1       0.89      0.96      0.93     19111\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     24013\n",
      "   macro avg       0.84      0.76      0.79     24013\n",
      "weighted avg       0.87      0.88      0.87     24013\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 2728  2174]\n",
      " [  778 18333]]\n",
      "Testing Score:\n",
      "0.8770665889309958\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from preprocess import Preprocessor\n",
    "# Scikit Learn Libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import metrics\n",
    "# XGBoost Library\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def random_forest(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    t0 = time.time()\n",
    "    clf = RandomForestClassifier(n_estimators=200, max_depth=12, random_state=0, min_samples_split=2, n_jobs=-1)\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    expected = y_test\n",
    "    predicted = clf.predict(X_test)\n",
    "    t1 = time.time()\n",
    "\n",
    "    print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "          % (clf, metrics.classification_report(expected, predicted)))\n",
    "    print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n",
    "    print (\"Testing Score:\")\n",
    "    print (clf.score(X_test,y_test))\n",
    "    print ('Time')\n",
    "    print (t1-t0)\n",
    "    print ('')\n",
    "    \n",
    "def logistic_regression(X_train, y_train, X_test, y_test):\n",
    "        \n",
    "    t0 = time.time()\n",
    "    clf = LogisticRegression(tol=1e-3, C=1.5, random_state=0)\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    expected = y_test\n",
    "    predicted = clf.predict(X_test)\n",
    "\n",
    "    print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "          % (clf, metrics.classification_report(expected, predicted)))\n",
    "    print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n",
    "    print (\"Testing Score:\")\n",
    "    print (clf.score(X_test,y_test))\n",
    "    print ('Time')\n",
    "    print (t1-t0)\n",
    "    print ('')\n",
    "    \n",
    "def XGboost(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    t0 = time.time()\n",
    "    clf = XGBClassifier(n_estimators=200, max_depth=6, learning_rate=0.05, min_child_weight=2, \n",
    "                        n_jobs=-1, max_delta_step=1, objective='binary:logistic', gamma=3 ,subsample=1)\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    expected = y_test\n",
    "    predicted = clf.predict(X_test)\n",
    "\n",
    "    print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "          % (clf, metrics.classification_report(expected, predicted)))\n",
    "    print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n",
    "    print (\"Testing Score:\")\n",
    "    print (clf.score(X_test,y_test))\n",
    "    print ('Time')\n",
    "    print (t1-t0)\n",
    "    print ('')\n",
    "    \n",
    "def voting(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    t0 = time.time()\n",
    "    clf1 = RandomForestClassifier(n_estimators=200, max_depth=12, random_state=0, min_samples_split=3, n_jobs=-1)\n",
    "    clf2 = LogisticRegression(tol=1e-3, C=1.5, random_state=0)\n",
    "    clf3 = XGBClassifier(n_estimators=200, max_depth=6, learning_rate=0.05, min_child_weight=2, \n",
    "                        n_jobs=-1, max_delta_step=1, objective='binary:logistic', gamma=3 ,subsample=1)\n",
    "    \n",
    "    clf = VotingClassifier(estimators=[('rf', clf1), ('lr', clf2), ('xgb', clf3)], voting='hard')\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    expected = y_test\n",
    "    predicted = clf.predict(X_test)\n",
    "\n",
    "    print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "          % (clf, metrics.classification_report(expected, predicted)))\n",
    "    print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n",
    "    print (\"Testing Score:\")\n",
    "    print (clf.score(X_test,y_test))\n",
    "    print ('Time')\n",
    "    print (t1-t0)\n",
    "    \n",
    "def main():\n",
    "    \n",
    "    # Build preprocessor\n",
    "    p_train = Preprocessor('train')\n",
    "    p_test = Preprocessor('test')\n",
    "    \n",
    "    # Get value\n",
    "    X_train, y_train = p_train.get_values_all()\n",
    "    X_test, y_test = p_test.get_values_all()\n",
    "    \n",
    "    print ('==========Classification==========')\n",
    "    # Random forest\n",
    "    random_forest(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # Logistic regression\n",
    "    logistic_regression(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # XGboost\n",
    "    XGboost(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    print ('==========Voting==========')\n",
    "    # Voting\n",
    "    voting(X_train, y_train, X_test, y_test)\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from preprocess import Preprocessor\n",
    "# Scikit Learn Libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "# XGBoost Library\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data/date.csv loaded! Number of courses:', 39)\n",
      "('data/object.csv loaded! Number of moduels:', 26750)\n",
      "('data/train/log_train.csv loaded! Size of log data:', 4677908)\n",
      "('data/train/enrollment_train.csv loaded! Number of enrollments:', 72395)\n",
      "('data/train/truth_train.csv loaded! Number of labels:', 72395)\n",
      "==========train data loading finished==========\n",
      "()\n",
      "('data/date.csv loaded! Number of courses:', 39)\n",
      "('data/object.csv loaded! Number of moduels:', 26750)\n",
      "('data/test/log_test.csv loaded! Size of log data:', 1548480)\n",
      "('data/test/enrollment_test.csv loaded! Number of enrollments:', 24013)\n",
      "('data/test/truth_test.csv loaded! Number of labels:', 24013)\n",
      "==========test data loading finished==========\n",
      "()\n",
      "Event_count extracted! Number of features: 7; time: 1.196238 seconds\n",
      "Weekly_session_count extracted! Number of features: 6; time: 66.593662 seconds\n",
      "Problem_video_ratio extracted! Number of features: 2; time: 3.366534 seconds\n",
      "==========All features extracted==========\n",
      "('Shape of the features dataframe: ', (72395, 16))\n",
      "Finish training data preprocessing! Time: 71.351416 seconds\n",
      "The shape of X: (72395, 15); shape of y: (72395,)\n",
      "()\n",
      "Event_count extracted! Number of features: 7; time: 0.446243 seconds\n",
      "Weekly_session_count extracted! Number of features: 6; time: 20.732689 seconds\n",
      "Problem_video_ratio extracted! Number of features: 2; time: 1.020373 seconds\n",
      "==========All features extracted==========\n",
      "('Shape of the features dataframe: ', (24013, 16))\n",
      "Finish testing data preprocessing! Time: 22.264347 seconds\n",
      "The shape of X: (24013, 15); shape of y: (24013,)\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "# Build preprocessor\n",
    "p_train = Preprocessor('train')\n",
    "p_test = Preprocessor('test')\n",
    "\n",
    "# Get value\n",
    "X_train, y_train = p_train.get_values_all()\n",
    "X_test, y_test = p_test.get_values_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score:\n",
      "[0.87327348 0.87126183 0.87395538 0.8691208  0.87256527]\n",
      "Cross Validation Time:\n",
      "194.506865978\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "clf = RandomForestClassifier(n_estimators=500,random_state=0)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "print 'Cross Validation Score:'\n",
    "print cross_val_score(clf, X_train, y_train, cv=5)\n",
    "t1 = time.time()\n",
    "print 'Cross Validation Time:'\n",
    "print t1 - t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score:\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "clf = svm.SVC(gamma='scale') \n",
    "clf = clf.fit(X_train, y_train)\n",
    "print 'Cross Validation Score:'\n",
    "print cross_val_score(clf, X_train, y_train, cv=5)\n",
    "t1 = time.time()\n",
    "print 'Cross Validation Time:'\n",
    "print t1 - t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score:\n",
      "[0.86754144 0.86732509 0.87126183 0.86097106 0.86800663]\n",
      "Cross Validation Time:\n",
      "472.91376996\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "clf = MLPClassifier(hidden_layer_sizes=(50,50))\n",
    "clf = clf.fit(X_train, y_train)\n",
    "print 'Cross Validation Score:'\n",
    "print cross_val_score(clf, X_train, y_train, cv=5)\n",
    "t1 = time.time()\n",
    "print 'Cross Validation Time:'\n",
    "print t1 - t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score:\n",
      "[0.87569061 0.87326473 0.87699427 0.8711237  0.8736704 ]\n",
      "Cross Validation Time:\n",
      "1.92082500458\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "clf = LogisticRegression()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "print 'Cross Validation Score:'\n",
    "print cross_val_score(clf, X_train, y_train, cv=5)\n",
    "t1 = time.time()\n",
    "print 'Cross Validation Time:'\n",
    "print t1 - t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score:\n",
      "[0.87672652 0.87416258 0.87885904 0.87215968 0.87394668]\n",
      "Cross Validation Time:\n",
      "29.6431109905\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "clf = GradientBoostingClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "print 'Cross Validation Score:'\n",
    "print cross_val_score(clf, X_train, y_train, cv=5)\n",
    "t1 = time.time()\n",
    "print 'Cross Validation Time:'\n",
    "print t1 - t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGboosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score:\n",
      "[0.87810773 0.8747151  0.87858278 0.87367912 0.87449924]\n",
      "Cross Validation Time:\n",
      "27.1639420986\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "clf = XGBClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "print 'Cross Validation Score:'\n",
    "print cross_val_score(clf, X_train, y_train, cv=5)\n",
    "t1 = time.time()\n",
    "print 'Cross Validation Time:'\n",
    "print t1 - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score:\n",
      "[0.87845304 0.87409351 0.87885904 0.87278127 0.8750518 ]\n",
      "Cross Validation Time:\n",
      "511.561968088\n"
     ]
    }
   ],
   "source": [
    "clf1 = RandomForestClassifier(n_estimators=500, random_state=0)\n",
    "clf2 = LogisticRegression()\n",
    "clf3 = GradientBoostingClassifier()\n",
    "clf = VotingClassifier(estimators=[('rf', clf1), ('lr', clf2), ('xgb', clf3)], voting='hard')\n",
    "clf = clf.fit(X_train, y_train)\n",
    "print 'Cross Validation Score:'\n",
    "print cross_val_score(clf, X_train, y_train, cv=5)\n",
    "t1 = time.time()\n",
    "print 'Cross Validation Time:'\n",
    "print t1 - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
